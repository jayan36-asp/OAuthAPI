{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpAtMXFdUpgUT7ozPA2jxE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayan36-asp/OAuthAPI/blob/master/NMTSITS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets sentencepiece\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the OPUS dataset (English-French)\n",
        "dataset = load_dataset(\"opus_books\", \"en-fr\")\n",
        "\n",
        "# Print a sample\n",
        "print(dataset[\"train\"][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Pw14EpulcRcU",
        "outputId": "353578a5-8fa8-4be7-aa69-aaf1a2fc35fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '0', 'translation': {'en': 'The Wanderer', 'fr': 'Le grand Meaulnes'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Save dataset to text files\n",
        "with open(\"train.en\", \"w\", encoding=\"utf-8\") as f_en, open(\"train.fr\", \"w\", encoding=\"utf-8\") as f_fr:\n",
        "    for pair in dataset[\"train\"]:\n",
        "        f_en.write(pair[\"translation\"][\"en\"] + \"\\n\")\n",
        "        f_fr.write(pair[\"translation\"][\"fr\"] + \"\\n\")\n",
        "\n",
        "# Train SentencePiece tokenizer\n",
        "spm.SentencePieceTrainer.train(input=\"train.en\", model_prefix=\"spm_en\", vocab_size=32000)\n",
        "spm.SentencePieceTrainer.train(input=\"train.fr\", model_prefix=\"spm_fr\", vocab_size=32000)\n",
        "\n",
        "# Load trained tokenizer\n",
        "sp_en = spm.SentencePieceProcessor(model_file=\"spm_en.model\")\n",
        "sp_fr = spm.SentencePieceProcessor(model_file=\"spm_fr.model\")\n",
        "\n",
        "# Example tokenization\n",
        "print(sp_en.encode(\"This is a test sentence.\", out_type=str))\n",
        "print(sp_fr.encode(\"Ceci est une phrase de test.\", out_type=str))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCAo9HwlcVf3",
        "outputId": "0fe3b71c-00d4-4642-9f5e-10c716d7e0c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁This', '▁is', '▁a', '▁test', '▁sentence', '.']\n",
            "['▁Ceci', '▁est', '▁une', '▁phrase', '▁de', '▁test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, dataset, src_tokenizer, trg_tokenizer, max_len=50):\n",
        "        self.data = dataset\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.trg_tokenizer = trg_tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = self.data[idx][\"translation\"][\"en\"]\n",
        "        trg_text = self.data[idx][\"translation\"][\"fr\"]\n",
        "\n",
        "        # Tokenize & add special tokens\n",
        "        src_tokens = [self.src_tokenizer.bos_id()] + self.src_tokenizer.encode(src_text)[:self.max_len] + [self.src_tokenizer.eos_id()]\n",
        "        trg_tokens = [self.trg_tokenizer.bos_id()] + self.trg_tokenizer.encode(trg_text)[:self.max_len] + [self.trg_tokenizer.eos_id()]\n",
        "\n",
        "        return torch.tensor(src_tokens), torch.tensor(trg_tokens)\n",
        "\n",
        "# Load dataset\n",
        "train_data = dataset[\"train\"]\n",
        "\n",
        "# Create dataset instance\n",
        "train_dataset = TranslationDataset(train_data, sp_en, sp_fr)\n"
      ],
      "metadata": {
        "id": "lrFeE7IKcXHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, trg_batch = zip(*batch)\n",
        "\n",
        "    # Pad sequences to the same length\n",
        "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
        "    trg_batch = pad_sequence(trg_batch, batch_first=True, padding_value=0)\n",
        "\n",
        "    return src_batch, trg_batch\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "dwoOnLr8cr18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, n_heads, ff_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.pos_encoding = nn.Parameter(torch.zeros(1, 500, emb_dim))  # Max length = 500\n",
        "        self.encoder_layers = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads, dim_feedforward=ff_dim, dropout=dropout),\n",
        "            num_layers\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src_emb = self.dropout(self.embedding(src) + self.pos_encoding[:, :src.size(1), :])\n",
        "        return self.encoder_layers(src_emb)\n"
      ],
      "metadata": {
        "id": "ulWLC7pBcxSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, n_heads, ff_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.pos_encoding = nn.Parameter(torch.zeros(1, 500, emb_dim))  # Max length = 500\n",
        "        self.decoder_layers = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model=emb_dim, nhead=n_heads, dim_feedforward=ff_dim, dropout=dropout, batch_first=True),\n",
        "            num_layers\n",
        "        )\n",
        "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, enc_output, tgt_mask=None):\n",
        "        trg_emb = self.dropout(self.embedding(trg) + self.pos_encoding[:, :trg.size(1), :])\n",
        "\n",
        "        # 🔥 Ensure tgt_mask is correctly shaped\n",
        "        if tgt_mask is None:\n",
        "            seq_len = trg.shape[1]  # Get sequence length\n",
        "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(trg.device)  # Correct mask size\n",
        "\n",
        "        output = self.decoder_layers(trg_emb, enc_output, tgt_mask)\n",
        "        return self.fc_out(output)\n"
      ],
      "metadata": {
        "id": "IFoJ-dtvc2Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerSeq2Seq(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, emb_dim, n_heads, ff_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.encoder = TransformerEncoder(input_dim, emb_dim, n_heads, ff_dim, num_layers, dropout)\n",
        "        self.decoder = TransformerDecoder(output_dim, emb_dim, n_heads, ff_dim, num_layers, dropout)\n",
        "\n",
        "    def forward(self, src, trg, tgt_mask):\n",
        "        enc_output = self.encoder(src)\n",
        "        return self.decoder(trg, enc_output, tgt_mask)\n"
      ],
      "metadata": {
        "id": "Nu6y-F7dc41o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the dataset using the trained SentencePiece models\n",
        "src_train = [sp_en.encode(pair[\"translation\"][\"en\"]) for pair in dataset[\"train\"]]\n",
        "trg_train = [sp_fr.encode(pair[\"translation\"][\"fr\"]) for pair in dataset[\"train\"]]\n",
        "\n",
        "# Print an example to verify\n",
        "print(f\"Example tokenized source: {src_train[0]}\")\n",
        "print(f\"Example tokenized target: {trg_train[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5gS6HUTdD2N",
        "outputId": "661e3886-b705-4de1-8b23-e55947606fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example tokenized source: [46, 25539, 293]\n",
            "Example tokenized target: [90, 175, 851]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src, trg):\n",
        "        self.src = src\n",
        "        self.trg = trg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src[idx], self.trg[idx]\n"
      ],
      "metadata": {
        "id": "du1BFKz6dB8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TranslationDataset(src_train, trg_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n"
      ],
      "metadata": {
        "id": "Mx9B_2J2dw7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"test\" not in dataset:\n",
        "    dataset = dataset[\"train\"].train_test_split(test_size=0.1)  # 10% for testing\n",
        "    dataset[\"validation\"] = dataset[\"test\"].train_test_split(test_size=0.5)[\"train\"]  # Half for validation\n",
        "print(dataset.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2b1JsC0fGtJ",
        "outputId": "bfcaa291-03ff-4201-8ac8-59d9de8c6e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['train', 'test', 'validation'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define model parameters\n",
        "INPUT_DIM = 32000  # Based on SentencePiece vocab size\n",
        "OUTPUT_DIM = 32000\n",
        "EMB_DIM = 256\n",
        "N_HEADS = 8\n",
        "FF_DIM = 512\n",
        "NUM_LAYERS = 3\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# Initialize the model\n",
        "model = TransformerSeq2Seq(INPUT_DIM, OUTPUT_DIM, EMB_DIM, N_HEADS, FF_DIM, NUM_LAYERS, DROPOUT).to(device)\n",
        "\n",
        "# Define optimizer & loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding token\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nemtGbbgmCe",
        "outputId": "fd419129-704f-4287-9218-f99e195130f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, trg in data_loader:\n",
        "        src = torch.nn.utils.rnn.pad_sequence([torch.tensor(s) for s in src], batch_first=True, padding_value=0).to(device)\n",
        "        trg = torch.nn.utils.rnn.pad_sequence([torch.tensor(t) for t in trg], batch_first=True, padding_value=0).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_input = trg[:, :-1]  # Teacher forcing: Remove last token\n",
        "        tgt_output = trg[:, 1:]  # Expected output: Shift left\n",
        "\n",
        "        seq_len = tgt_input.shape[1]  # Get actual sequence length\n",
        "\n",
        "        # 🔹 Dynamically generate correct attn_mask\n",
        "        tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(seq_len).to(device)\n",
        "\n",
        "        # 🔹 Ensure the mask shape is dynamically adjusted\n",
        "        if tgt_mask.shape != (seq_len, seq_len):\n",
        "            tgt_mask = tgt_mask[:seq_len, :seq_len]\n",
        "\n",
        "        output = model(src, tgt_input, tgt_mask)\n",
        "\n",
        "        loss = criterion(output.reshape(-1, OUTPUT_DIM), tgt_output.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(data_loader)\n"
      ],
      "metadata": {
        "id": "duTNxErxg-K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 1\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion)\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "aK7z3jRHgotF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}